{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install clean-text\n",
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk import sent_tokenize\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from functools import partial\n",
    "from cleantext import clean\n",
    "from gensim.parsing.preprocessing import DEFAULT_FILTERS\n",
    "import matplotlib.pylab as plt\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_meta_df = pd.read_csv(\"../datasets/raw/spotify/spotify-podcasts-2020/metadata.tsv\", sep=\"\\t\", )\n",
    "print(\"##Column names## : \", \", \".join(spotify_meta_df.columns))\n",
    "# spotify_meta_df = spotify_meta_df.repartition(npartitions=spotify_meta_df.npartitions // 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaner = partial(clean,\n",
    "    fix_unicode=True,               # fix various unicode errors\n",
    "    to_ascii=True,                  # transliterate to closest ASCII representation\n",
    "    lower=True,                     # lowercase text\n",
    "    no_line_breaks=False,           # fully strip line breaks as opposed to only normalizing them\n",
    "    no_urls=True,                  # replace all URLs with a special token\n",
    "    no_emails=True,                # replace all email addresses with a special token\n",
    "    no_phone_numbers=True,         # replace all phone numbers with a special token\n",
    "    no_numbers=True,               # replace all numbers with a special token\n",
    "    no_digits=True,                # replace all digits with a special token\n",
    "    no_currency_symbols=True,      # replace all currency symbols with a special token\n",
    "    no_punct=True,                 # remove punctuations\n",
    "    lang=\"en\"                       # set to 'de' for German special handling\n",
    ")\n",
    "filters = [cleaner] + DEFAULT_FILTERS\n",
    "string_preprocessor=partial(preprocess_string, filters=filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eposide_sentences = (\n",
    "    spotify_meta_df[\"episode_description\"]\n",
    "    .dropna()\n",
    "    .map(lambda x: sent_tokenize(x) if isinstance(x, str) else None)\n",
    "    .explode()\n",
    "    .dropna()\n",
    "    .to_frame()\n",
    "    )\n",
    "\n",
    "eposide_sentences[\"preprocessed_sentences\"] = eposide_sentences[\"episode_description\"].map(string_preprocessor)\n",
    "eposide_sentences[\"preprocessed_sentences\"] = (\n",
    "    eposide_sentences[\"preprocessed_sentences\"]\n",
    "    .map(lambda x: x if isinstance(x, list) and len(x) > 0 else None))\n",
    "eposide_sentences = eposide_sentences.dropna(subset=[\"preprocessed_sentences\"])\n",
    "eposide_sentences[\"preprocessed_sentences\"] = eposide_sentences[\"preprocessed_sentences\"].map(lambda x: \" \".join(x))\n",
    "eposide_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(binary=True, min_df=5,max_df=0.85, norm=False)\n",
    "vectorizer = vectorizer.fit(eposide_sentences[\"preprocessed_sentences\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vecs = vectorizer.transform(eposide_sentences[\"preprocessed_sentences\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = eposide_sentences.iloc[sentence_vecs.sum(axis=1).A1 > 25]\n",
    "filtered_descriptions = (\n",
    "    filtered_sentences\n",
    "    .groupby(filtered_sentences.index)\n",
    "    .agg({\"episode_description\": list})\n",
    "    .episode_description\n",
    "    .map(lambda x: x if isinstance(x, list) and len(x)>0 else None)\n",
    "    .dropna()\n",
    "    )\n",
    "filtered_descriptions.iloc[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = eposide_sentences.iloc[sentence_vecs.sum(axis=1).A1 > 25]\n",
    "filtered_sentences.iloc[-1][\"episode_description\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotify_meta_df[\"filtered_descriptions\"] = filtered_descriptions\n",
    "spotify_meta_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    spotify_meta_df[[\"episode_filename_prefix\", \"filtered_descriptions\"]]\n",
    "    .dropna()\n",
    "    .rename(\n",
    "        {\"episode_filename_prefix\": \"file_prefix\",\n",
    "         \"filtered_descriptions\": \"summary\"\n",
    "         }, axis=1)\n",
    ").to_json(\"../datasets/raw/spotify/clean_summaries.json\", lines=True, orient=\"records\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_transcript(item):\n",
    "    utterances = []\n",
    "    results = json.load(item.open())\n",
    "    results = results.get(\"results\")\n",
    "    if results is not None:\n",
    "        for result in results:\n",
    "            alternatives = result.get(\"alternatives\")\n",
    "            if alternatives:\n",
    "                transcript = alternatives[0].get(\"transcript\")\n",
    "                if transcript:\n",
    "                    utterances.append(transcript)\n",
    "\n",
    "    file_prefix = item.stem\n",
    "    data = {\"document\": utterances, \"file_prefix\": file_prefix}\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from multiprocessing import Pool, cpu_count\n",
    "pool = Pool(cpu_count()-1)\n",
    "import pathlib\n",
    "\n",
    "transcripts = pathlib.Path(\"../datasets/raw/spotify/spotify-podcasts-2020/podcasts-transcripts\").glob(\"**/*.json\")\n",
    "transcripts = list(transcripts)\n",
    "with open(\"../datasets/raw/spotify/transcripts.json\", \"w+\") as outfile:\n",
    "    loaded_transcripts = pool.imap_unordered(load_transcript, transcripts)\n",
    "    for item in tqdm(loaded_transcripts, total=len(transcripts)):\n",
    "        outfile.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "transcripts_df = dd.read_json(\"../datasets/raw/spotify/transcripts.json\", lines=True, orient=\"records\").set_index(\"file_prefix\")\n",
    "summaries_df = dd.read_json(\"../datasets/raw/spotify/clean_summaries.json\", lines=True, orient=\"records\").set_index(\"file_prefix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df[\"summary\"]= summaries_df[\"summary\"]\n",
    "transcripts_df = transcripts_df.dropna()\n",
    "transcripts_df = transcripts_df.persist()\n",
    "transcripts_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcripts_df.to_json(\"../datasets/raw/spotify/podcast_dataset.json\", lines=True, orient=\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import train_test_split as dtts\n",
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "pbar = ProgressBar()\n",
    "pbar.register()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 53.3s\n",
      "[                                        ] | 0% Completed |  0.1s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bebop/anaconda3/envs/ac215/lib/python3.7/site-packages/dask_ml/model_selection/_split.py:469: FutureWarning: The default value for 'shuffle' must be specified when splitting DataFrames. In the future DataFrames will automatically be shuffled within blocks prior to splitting. Specify 'shuffle=True' to adopt the future behavior now, or 'shuffle=False' to retain the previous behavior.\n",
      "  category=FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  2min 59.3s\n",
      "[########################################] | 100% Completed |  1min 31.3s\n",
      "[########################################] | 100% Completed |  1min 44.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['/Users/bebop/Documents/courses/APCOMP215/AC215_projectgarble/notebooks/../datasets/raw/spotify/test/0.part']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = dd.read_json(\"../datasets/raw/spotify/podcast_dataset.json\", lines=True, orient=\"records\")\n",
    "\n",
    "train_df, test_df = dtts(data, test_size=0.2, shuffle=True)\n",
    "test_df, val_df = dtts(test_df, test_size=0.5, shuffle=True)\n",
    "\n",
    "train_df.to_json(\"../datasets/raw/spotify/train\")\n",
    "val_df.to_json(\"../datasets/raw/spotify/valid\")\n",
    "test_df.to_json(\"../datasets/raw/spotify/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gzip -c train.json > train.json.gz\n",
    "gzip -c valid.json > valid.json.gz\n",
    "gzip -c test.json > test.json.gz"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "288a62494f976085112cb18b35ded3e01e34d252161877d3238d46211efe281c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('ac215': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
