{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pathlib\n",
    "from collections import defaultdict\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##https://medium.com/swlh/text-summarization-guide-exploratory-data-analysis-on-text-data-4e22ce2dd6ad\n",
    "\n",
    "def count_sentences(examples):\n",
    "    return {\n",
    "        \"document_sentence_count\": [len(document) for document in examples[\"document\"]],\n",
    "        \"summary_sentence_count\": [len(document) for document in examples[\"summary\"]],\n",
    "    }\n",
    "\n",
    "def count_words(examples):\n",
    "    return {\n",
    "        \"document_word_count\": [sum(len(item.split()) for item in document) for document in examples[\"document\"]],\n",
    "        \"summary_word_count\": [sum(len(item.split()) for item in document) for document in examples[\"summary\"]],\n",
    "    }\n",
    "\n",
    "def count_chars(examples):\n",
    "    return {\n",
    "        \"document_char_count\": [sum(len(\"\".join(item.split())) for item in document) for document in examples[\"document\"]],\n",
    "        \"summary_char_count\": [sum(len(\"\".join(item.split())) for item in document) for document in examples[\"summary\"]],\n",
    "    }\n",
    "\n",
    "def sentence_density(examples):\n",
    "    return {\n",
    "        \"document_sentence_density\": [document[0]/(document[1] if document[1] else 1) for document in zip(examples[\"document_sentence_count\"], examples[\"document_word_count\"])], \n",
    "        \"summary_sentence_density\": [document[0]/(document[1] if document[1] else 1)  for document in zip(examples[\"summary_sentence_count\"], examples[\"summary_word_count\"])],\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "def _count_stopwords(text, stopwords=STOPWORDS):\n",
    "    ''' Return the number of stopwords in the text\n",
    "        Input:\n",
    "            - text: string\n",
    "            - stopwords: list of string, containing the stopwords\n",
    "        Output:\n",
    "            - int, number of stopwords in the text argument\n",
    "    '''\n",
    "    stopwords_x = [w for w in \"\\n\".join(text).split() if w.lower() in stopwords]\n",
    "    \n",
    "    return len(stopwords_x)\n",
    "\n",
    "def count_stopwords(examples):\n",
    "    return {\n",
    "        \"document_stopword_count\": [_count_stopwords(document) for document in examples[\"document\"]],\n",
    "        \"summary_stopword_count\": [_count_stopwords(document) for document in examples[\"summary\"]]\n",
    "    }\n",
    "\n",
    "\n",
    "def load_dataset_stats(dataset):\n",
    "\n",
    "    features = datasets.features.Features({\n",
    "    'document': datasets.Sequence(feature=datasets.Value(dtype='string', id=None), length=-1, id=None),\n",
    "    'summary': datasets.Sequence(feature=datasets.Value(dtype='string', id=None), length=-1, id=None)\n",
    "    })\n",
    "    dataset_name, splits = dataset\n",
    "\n",
    "    dataset = datasets.load_dataset(\"json\", dataset_name, data_files=splits, features=features,)\n",
    "    removed_cols = list(set(column_name for sublist in dataset.values() for column_name in sublist.column_names))\n",
    "\n",
    "    datasets_info = (\n",
    "        dataset\n",
    "        .map(count_sentences,batched=True)\n",
    "        .map(count_words,batched=True,)\n",
    "        .map(count_chars, batched=True)\n",
    "        .map(sentence_density,batched=True)\n",
    "        .map(count_stopwords, batched=True, remove_columns=removed_cols)\n",
    "        )\n",
    "    \n",
    "    return datasets_info\n",
    "\n",
    "\n",
    "def load_data_splits_from_dir(directory):\n",
    "    splits = defaultdict(dict)\n",
    "    directory_files = pathlib.Path(directory).glob(\"**/*.json.gz\")\n",
    "    for file_path in directory_files:\n",
    "        data_split = file_path.stem.split(\".\")[0]\n",
    "        splits[file_path.parent.stem][data_split] = str(file_path.resolve())\n",
    "    return dict(splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b4827134a9e483e9bf1d1640fdaca1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration ami-1844cbad6c43206f\n",
      "Reusing dataset json (/Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50)\n",
      "100%|██████████| 3/3 [00:00<00:00, 423.97it/s]\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-77fe406a0c73db71.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-01f102143ae717bf.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-32388734c206c20d.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-87e2ec7f98872068.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-8a79564374a1efb5.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-ca9af9ed3940b4c3.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-a69876978bf1f5c8.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-0ae8ddcec6e4d034.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-f5e8123975462bfd.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-a25b43787216689f.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-7552c2c472e42a6b.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-00105d28038bc4ee.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-6c99c3dfa3e5694c.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-4d6265e998cd711d.arrow\n",
      "Loading cached processed dataset at /Users/bebop/.cache/huggingface/datasets/json/ami-1844cbad6c43206f/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-a477c23ef01e1886.arrow\n",
      "Using custom data configuration ted-dae48c31dc58aa01\n",
      "Reusing dataset json (/Users/bebop/.cache/huggingface/datasets/json/ted-dae48c31dc58aa01/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50)\n",
      "100%|██████████| 3/3 [00:00<00:00, 264.71it/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.61ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.79ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  5.04ba/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.72ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.58ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.66ba/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  2.83ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.23ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  4.21ba/s]\n",
      "100%|██████████| 3/3 [00:00<00:00,  6.40ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  9.79ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  8.22ba/s]\n",
      "100%|██████████| 3/3 [00:01<00:00,  1.97ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.87ba/s]\n",
      "100%|██████████| 1/1 [00:00<00:00,  2.80ba/s]\n",
      "Using custom data configuration cc_news-f49c84cd4eaf147c\n",
      "Reusing dataset json (/Users/bebop/.cache/huggingface/datasets/json/cc_news-f49c84cd4eaf147c/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50)\n",
      "100%|██████████| 3/3 [00:00<00:00, 15.74it/s]\n"
     ]
    }
   ],
   "source": [
    "RAW_SUMMARIZATION_DATASETS_DIR = \"../datasets/raw/supervised/summarization\"\n",
    "data_splits = load_data_splits_from_dir(RAW_SUMMARIZATION_DATASETS_DIR)\n",
    "for dataset in tqdm(data_splits.items()):\n",
    "    ## TODO Write Dataset loading script\n",
    "    # https://huggingface.co/docs/datasets/v1.12.0/dataset_script.html\n",
    "    dataset_info = load_dataset_stats(dataset)\n",
    "    dataset_name, _ = dataset\n",
    "    for split_name,split_info in dataset_info.items():\n",
    "        outfile = pathlib.Path(RAW_SUMMARIZATION_DATASETS_DIR).joinpath(dataset_name, f\"{split_name}_info.csv\").resolve()\n",
    "        split_df = split_info.to_pandas()\n",
    "        split_df.to_csv(outfile, index=False, index_label=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "288a62494f976085112cb18b35ded3e01e34d252161877d3238d46211efe281c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('ac215': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
